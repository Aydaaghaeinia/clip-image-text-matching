{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okOp2UWPyc6C",
        "outputId": "52a6e182-56fc-4ad5-8cfa-ec2932e7159b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-hnj04wr_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-hnj04wr_\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install git+https://github.com/openai/CLIP.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# بارگذاری مدل CLIP و پیش‌پردازشگر آن\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device)\n"
      ],
      "metadata": {
        "id": "Z-8lKkFwyi4P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# بارگذاری تصویر\n",
        "image = preprocess(Image.open(\"/content/imag3.jpg\")).unsqueeze(0).to(device)\n"
      ],
      "metadata": {
        "id": "jo0IT20AzoK9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# تعریف جملات برای تطابق\n",
        "text = [\"A dog playing with a blue ball\"]\n",
        "\n",
        "# تبدیل متن به ویژگی‌های عددی\n",
        "text_inputs = torch.cat([clip.tokenize(t) for t in text]).to(device)\n",
        "\n",
        "# انجام پیش‌بینی تطابق\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "# محاسبه تطابق\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (image_features @ text_features.T).squeeze(0)\n",
        "\n",
        "# نمایش نتایج\n",
        "values, indices = similarity.topk(1)\n",
        "print(f\"تصویر به بهترین توصیف: {text[indices[0]]} با شباهت {values[0]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ia3Ym0G4fpF",
        "outputId": "a26692af-8765-416c-afdc-a135d2b2e2b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تصویر به بهترین توصیف: A dog playing with a blue ball با شباهت 0.3229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A brown dog chasing a blue ball"
      ],
      "metadata": {
        "id": "jLMERc7E8KkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title بارگذاری مدل و پردازش تصویر\n",
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Step 2: Load a more powerful CLIP model\n",
        "# We're using \"ViT-L/14\" which is larger and more accurate than \"ViT-B/32\".\n",
        "# The model is moved to the GPU (\"cuda\") if available, for faster processing.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# مدل قوی‌تر ViT-L/14 را بارگذاری می‌کنیم که دقت بالاتری دارد\n",
        "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
        "print(\"Model ViT-L/14 loaded successfully.\")\n",
        "\n",
        "# Step 3: Load and preprocess the image\n",
        "# The image path is specified here. Make sure the file exists.\n",
        "# The 'preprocess' function standardizes the image (resizing, normalization)\n",
        "# for the CLIP model.\n",
        "image_path = \"/content/imag3.jpg\"\n",
        "\n",
        "if not os.path.exists(image_path):\n",
        "    print(f\"خطا: فایل تصویر در مسیر '{image_path}' یافت نشد.\")\n",
        "    print(\"لطفاً تصویر خود را آپلود کرده و مسیر آن را به درستی وارد کنید.\")\n",
        "else:\n",
        "    # بارگذاری تصویر\n",
        "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "    print(\"Image loaded and preprocessed successfully.\")\n",
        "\n",
        "    # Step 4: Define a more diverse and descriptive set of text prompts\n",
        "    # Providing multiple, varied descriptions helps the model make a more accurate choice.\n",
        "    # We include specific details and also some incorrect descriptions to test the model's accuracy.\n",
        "    text_descriptions = [\n",
        "        \"A photo of a brown dog playing with a blue ball in the grass\", # توصیف دقیق\n",
        "        \"A golden retriever chasing a ball outdoors\", # توصیف نزدیک\n",
        "        \"A pet enjoying a sunny day\", # توصیف کلی\n",
        "        \"There is a dog and a ball in the image\", # توصیف مبتنی بر اشیاء\n",
        "        \"A cat sleeping on a red sofa\", # توصیف کاملا نامرتبط\n",
        "        \"A landscape view of mountains at sunset\" # توصیف نامرتبط دیگر\n",
        "    ]\n",
        "\n",
        "    # Step 5: Tokenize the text descriptions\n",
        "    # 'clip.tokenize' converts the text strings into numerical tokens that the model can understand.\n",
        "    text_inputs = clip.tokenize(text_descriptions).to(device)\n",
        "\n",
        "    # Step 6: Generate features and calculate similarity\n",
        "    # We disable gradient calculations for efficiency as we are only doing inference.\n",
        "    with torch.no_grad():\n",
        "        # The model encodes both the image and the text prompts into feature vectors.\n",
        "        image_features = model.encode_image(image)\n",
        "        text_features = model.encode_text(text_inputs)\n",
        "\n",
        "        # Normalize the features to have a unit length. This is crucial for cosine similarity.\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Calculate the cosine similarity between the image features and all text features.\n",
        "        # We convert the result to probabilities using softmax.\n",
        "        similarity_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "    # Step 7: Display the results\n",
        "    # We get the top 3 most likely descriptions for the image.\n",
        "    values, indices = similarity_probs[0].topk(3)\n",
        "\n",
        "    print(\"\\n--- نتایج تطابق تصویر با توصیف‌ها ---\")\n",
        "    print(\"مدل، تصویر شما را با توصیف‌های زیر مقایسه کرده و این نتایج را برگردانده است:\\n\")\n",
        "\n",
        "    for i, (value, index) in enumerate(zip(values, indices)):\n",
        "        print(f\"#{i+1}: \\\"{text_descriptions[index]}\\\"\")\n",
        "        print(f\"   - احتمال تطابق: {value.item() * 100:.2f}%\")\n",
        "        print(\"-\" * 20)\n"
      ],
      "metadata": {
        "id": "mILe9A6Q_OrR",
        "outputId": "f8a4f935-2f0c-4d4a-859a-991e301ac412",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 890M/890M [00:57<00:00, 16.2MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model ViT-L/14 loaded successfully.\n",
            "Image loaded and preprocessed successfully.\n",
            "\n",
            "--- نتایج تطابق تصویر با توصیف‌ها ---\n",
            "مدل، تصویر شما را با توصیف‌های زیر مقایسه کرده و این نتایج را برگردانده است:\n",
            "\n",
            "#1: \"A photo of a brown dog playing with a blue ball in the grass\"\n",
            "   - احتمال تطابق: 71.69%\n",
            "--------------------\n",
            "#2: \"A golden retriever chasing a ball outdoors\"\n",
            "   - احتمال تطابق: 27.24%\n",
            "--------------------\n",
            "#3: \"There is a dog and a ball in the image\"\n",
            "   - احتمال تطابق: 0.95%\n",
            "--------------------\n"
          ]
        }
      ]
    }
  ]
}